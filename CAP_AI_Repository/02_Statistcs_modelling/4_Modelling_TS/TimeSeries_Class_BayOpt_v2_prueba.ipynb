{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# LIBRARIES \n",
    "##################################################\n",
    "%reset -f\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re, sqlite3, pickle, time, datetime, random, sys\n",
    "# sys.path is a list of absolute path strings\n",
    "sys.path.append('/home/d/dlr10/Documents/02_Statitics_modelling/0_FunctionsScripts')\n",
    "import Loading_Data_Functions as load_fn\n",
    "import FineTuning_Functions as FineTuning\n",
    "import Learning_Curves_Functions as LearningCurves\n",
    "\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/minirocket-fast-er-and-accurate-time-series-classification-cdacca2dcbfa\n",
    "# https://towardsdatascience.com/sktime-a-unified-python-library-for-time-series-machine-learning-3c103c139a55\n",
    "# https://pypi.org/project/sktime/\n",
    "# https://arxiv.org/pdf/1302.2277.pdf\n",
    "# https://github.com/alan-turing-institute/sktime/tree/main/sktime/transformations/panel/rocket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================FIRST RECORDED SAMPLE ADMISSION=================================\n",
      "-------------------------------------- 2016 - 2018 ----------------------------------------------\n",
      "7657\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admission_id</th>\n",
       "      <th>rr</th>\n",
       "      <th>ews</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>temperature</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>Oxygen_Saturation</th>\n",
       "      <th>Assisted_O2</th>\n",
       "      <th>Confusion</th>\n",
       "      <th>CREA</th>\n",
       "      <th>UREA</th>\n",
       "      <th>K</th>\n",
       "      <th>GFR</th>\n",
       "      <th>WBC</th>\n",
       "      <th>PLT</th>\n",
       "      <th>HCT</th>\n",
       "      <th>HGB</th>\n",
       "      <th>RBC</th>\n",
       "      <th>MCH</th>\n",
       "      <th>MCV</th>\n",
       "      <th>NEUAB</th>\n",
       "      <th>TLYMAB</th>\n",
       "      <th>EOSAB</th>\n",
       "      <th>MONAB</th>\n",
       "      <th>BASAB</th>\n",
       "      <th>ALB</th>\n",
       "      <th>ALP</th>\n",
       "      <th>BILI</th>\n",
       "      <th>no_sample_series</th>\n",
       "      <th>sex</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>age_at_admin</th>\n",
       "      <th>Comorb_score</th>\n",
       "      <th>Spcfc_Comorb</th>\n",
       "      <th>Mortality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30454500</td>\n",
       "      <td>16.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>143.000</td>\n",
       "      <td>36.400</td>\n",
       "      <td>81.000</td>\n",
       "      <td>49.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>76.000</td>\n",
       "      <td>17.350</td>\n",
       "      <td>4.300</td>\n",
       "      <td>69.000</td>\n",
       "      <td>6.500</td>\n",
       "      <td>201.500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>81.000</td>\n",
       "      <td>3.130</td>\n",
       "      <td>25.800</td>\n",
       "      <td>80.500</td>\n",
       "      <td>5.525</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.020</td>\n",
       "      <td>30.000</td>\n",
       "      <td>120.500</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30454500</td>\n",
       "      <td>15.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>115.667</td>\n",
       "      <td>36.500</td>\n",
       "      <td>88.667</td>\n",
       "      <td>62.333</td>\n",
       "      <td>92.667</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>76.000</td>\n",
       "      <td>17.350</td>\n",
       "      <td>4.300</td>\n",
       "      <td>69.000</td>\n",
       "      <td>6.500</td>\n",
       "      <td>201.500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>81.000</td>\n",
       "      <td>3.130</td>\n",
       "      <td>25.800</td>\n",
       "      <td>80.500</td>\n",
       "      <td>5.525</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.020</td>\n",
       "      <td>30.000</td>\n",
       "      <td>120.500</td>\n",
       "      <td>8.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30454500</td>\n",
       "      <td>14.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>88.333</td>\n",
       "      <td>36.600</td>\n",
       "      <td>96.333</td>\n",
       "      <td>75.667</td>\n",
       "      <td>95.333</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>76.000</td>\n",
       "      <td>17.350</td>\n",
       "      <td>4.300</td>\n",
       "      <td>69.000</td>\n",
       "      <td>6.500</td>\n",
       "      <td>201.500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>81.000</td>\n",
       "      <td>3.130</td>\n",
       "      <td>25.800</td>\n",
       "      <td>80.500</td>\n",
       "      <td>5.525</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.020</td>\n",
       "      <td>30.000</td>\n",
       "      <td>120.500</td>\n",
       "      <td>8.000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  admission_id     rr   ews  heart_rate  temperature    sbp    dbp  \\\n",
       "0     30454500 16.000 9.000     143.000       36.400 81.000 49.000   \n",
       "1     30454500 15.000 7.000     115.667       36.500 88.667 62.333   \n",
       "2     30454500 14.000 5.000      88.333       36.600 96.333 75.667   \n",
       "\n",
       "   Oxygen_Saturation  Assisted_O2  Confusion   CREA   UREA     K    GFR   WBC  \\\n",
       "0             90.000        1.000      1.000 76.000 17.350 4.300 69.000 6.500   \n",
       "1             92.667        1.000      1.000 76.000 17.350 4.300 69.000 6.500   \n",
       "2             95.333        1.000      1.000 76.000 17.350 4.300 69.000 6.500   \n",
       "\n",
       "      PLT   HCT    HGB   RBC    MCH    MCV  NEUAB  TLYMAB  EOSAB  MONAB  \\\n",
       "0 201.500 0.250 81.000 3.130 25.800 80.500  5.525   0.640  0.025  0.270   \n",
       "1 201.500 0.250 81.000 3.130 25.800 80.500  5.525   0.640  0.025  0.270   \n",
       "2 201.500 0.250 81.000 3.130 25.800 80.500  5.525   0.640  0.025  0.270   \n",
       "\n",
       "   BASAB    ALB     ALP  BILI no_sample_series   sex  ethnicity  age_at_admin  \\\n",
       "0  0.020 30.000 120.500 8.000                0 1.000      0.000        72.000   \n",
       "1  0.020 30.000 120.500 8.000                1 1.000      0.000        72.000   \n",
       "2  0.020 30.000 120.500 8.000                2 1.000      0.000        72.000   \n",
       "\n",
       "   Comorb_score  Spcfc_Comorb  Mortality  \n",
       "0         0.000         1.000      0.000  \n",
       "1         0.000         1.000      0.000  \n",
       "2         0.000         1.000      0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------- 2019 - 2020 ----------------------------------------------\n",
      "6531\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admission_id</th>\n",
       "      <th>rr</th>\n",
       "      <th>ews</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>temperature</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>Oxygen_Saturation</th>\n",
       "      <th>Assisted_O2</th>\n",
       "      <th>Confusion</th>\n",
       "      <th>CREA</th>\n",
       "      <th>UREA</th>\n",
       "      <th>K</th>\n",
       "      <th>GFR</th>\n",
       "      <th>WBC</th>\n",
       "      <th>PLT</th>\n",
       "      <th>HCT</th>\n",
       "      <th>HGB</th>\n",
       "      <th>RBC</th>\n",
       "      <th>MCH</th>\n",
       "      <th>MCV</th>\n",
       "      <th>NEUAB</th>\n",
       "      <th>TLYMAB</th>\n",
       "      <th>EOSAB</th>\n",
       "      <th>MONAB</th>\n",
       "      <th>BASAB</th>\n",
       "      <th>ALB</th>\n",
       "      <th>ALP</th>\n",
       "      <th>BILI</th>\n",
       "      <th>no_sample_series</th>\n",
       "      <th>sex</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>age_at_admin</th>\n",
       "      <th>Comorb_score</th>\n",
       "      <th>Spcfc_Comorb</th>\n",
       "      <th>Mortality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58578756</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>110.000</td>\n",
       "      <td>38.400</td>\n",
       "      <td>147.000</td>\n",
       "      <td>92.000</td>\n",
       "      <td>97.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>8.700</td>\n",
       "      <td>4.600</td>\n",
       "      <td>75.000</td>\n",
       "      <td>5.600</td>\n",
       "      <td>174.000</td>\n",
       "      <td>0.384</td>\n",
       "      <td>125.000</td>\n",
       "      <td>4.210</td>\n",
       "      <td>29.600</td>\n",
       "      <td>91.000</td>\n",
       "      <td>4.230</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.080</td>\n",
       "      <td>45.000</td>\n",
       "      <td>137.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58578756</td>\n",
       "      <td>19.619</td>\n",
       "      <td>2.000</td>\n",
       "      <td>108.190</td>\n",
       "      <td>38.343</td>\n",
       "      <td>145.762</td>\n",
       "      <td>90.667</td>\n",
       "      <td>96.905</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>8.700</td>\n",
       "      <td>4.600</td>\n",
       "      <td>75.000</td>\n",
       "      <td>5.600</td>\n",
       "      <td>174.000</td>\n",
       "      <td>0.384</td>\n",
       "      <td>125.000</td>\n",
       "      <td>4.210</td>\n",
       "      <td>29.600</td>\n",
       "      <td>91.000</td>\n",
       "      <td>4.230</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.080</td>\n",
       "      <td>45.000</td>\n",
       "      <td>137.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58578756</td>\n",
       "      <td>19.238</td>\n",
       "      <td>2.000</td>\n",
       "      <td>106.381</td>\n",
       "      <td>38.286</td>\n",
       "      <td>144.524</td>\n",
       "      <td>89.333</td>\n",
       "      <td>96.810</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>94.750</td>\n",
       "      <td>8.669</td>\n",
       "      <td>4.599</td>\n",
       "      <td>75.156</td>\n",
       "      <td>5.591</td>\n",
       "      <td>173.385</td>\n",
       "      <td>0.385</td>\n",
       "      <td>125.240</td>\n",
       "      <td>4.219</td>\n",
       "      <td>29.596</td>\n",
       "      <td>91.010</td>\n",
       "      <td>4.201</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.079</td>\n",
       "      <td>44.958</td>\n",
       "      <td>137.073</td>\n",
       "      <td>11.969</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  admission_id     rr   ews  heart_rate  temperature     sbp    dbp  \\\n",
       "0     58578756 20.000 2.000     110.000       38.400 147.000 92.000   \n",
       "1     58578756 19.619 2.000     108.190       38.343 145.762 90.667   \n",
       "2     58578756 19.238 2.000     106.381       38.286 144.524 89.333   \n",
       "\n",
       "   Oxygen_Saturation  Assisted_O2  Confusion   CREA  UREA     K    GFR   WBC  \\\n",
       "0             97.000        0.000      0.000 95.000 8.700 4.600 75.000 5.600   \n",
       "1             96.905        0.000      0.000 95.000 8.700 4.600 75.000 5.600   \n",
       "2             96.810        0.000      0.000 94.750 8.669 4.599 75.156 5.591   \n",
       "\n",
       "      PLT   HCT     HGB   RBC    MCH    MCV  NEUAB  TLYMAB  EOSAB  MONAB  \\\n",
       "0 174.000 0.384 125.000 4.210 29.600 91.000  4.230   0.800  0.030  0.400   \n",
       "1 174.000 0.384 125.000 4.210 29.600 91.000  4.230   0.800  0.030  0.400   \n",
       "2 173.385 0.385 125.240 4.219 29.596 91.010  4.201   0.798  0.030  0.398   \n",
       "\n",
       "   BASAB    ALB     ALP   BILI no_sample_series    sex  ethnicity  \\\n",
       "0  0.080 45.000 137.000 12.000                0 -1.000      0.000   \n",
       "1  0.080 45.000 137.000 12.000                1 -1.000      0.000   \n",
       "2  0.079 44.958 137.073 11.969                2 -1.000      0.000   \n",
       "\n",
       "   age_at_admin  Comorb_score  Spcfc_Comorb  Mortality  \n",
       "0        59.000         6.000         1.000      0.000  \n",
       "1        59.000         6.000         1.000      0.000  \n",
       "2        59.000         6.000         1.000      0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 23.60969042778015\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "path = r'/home/d/dlr10/Documents/02_Statitics_modelling/DataSets/'\n",
    "\n",
    "df_patients_16_18, df_admissions_16_18, df_eobs_16_18 = load_fn.Load_data()\n",
    "df_patients_19_20, df_admissions_19_20, df_eobs_19_20 = load_fn.Load_data('2019_2020')\n",
    "\n",
    "X_data_16_18 = pickle.load(open(path + 'df_ts_2016_18.pickle','rb'))\n",
    "X_data_19_20 = pickle.load(open(path + 'df_ts_2019_20.pickle','rb'))\n",
    "\n",
    "print(\"=================================FIRST RECORDED SAMPLE ADMISSION=================================\")\n",
    "print(\"-------------------------------------- 2016 - 2018 ----------------------------------------------\")\n",
    "print(len(X_data_16_18['admission_id'].unique().tolist()))\n",
    "display(X_data_16_18.head(3))\n",
    "print(\"\")\n",
    "print(\"-------------------------------------- 2019 - 2020 ----------------------------------------------\")\n",
    "print(len(X_data_19_20['admission_id'].unique().tolist()))\n",
    "display(X_data_19_20.head(3))\n",
    "\n",
    "print(\"Elapsed time:\", time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admission_id</th>\n",
       "      <th>rr</th>\n",
       "      <th>ews</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>temperature</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>Oxygen_Saturation</th>\n",
       "      <th>Assisted_O2</th>\n",
       "      <th>Confusion</th>\n",
       "      <th>CREA</th>\n",
       "      <th>UREA</th>\n",
       "      <th>K</th>\n",
       "      <th>GFR</th>\n",
       "      <th>WBC</th>\n",
       "      <th>PLT</th>\n",
       "      <th>HCT</th>\n",
       "      <th>HGB</th>\n",
       "      <th>RBC</th>\n",
       "      <th>MCH</th>\n",
       "      <th>MCV</th>\n",
       "      <th>NEUAB</th>\n",
       "      <th>TLYMAB</th>\n",
       "      <th>EOSAB</th>\n",
       "      <th>MONAB</th>\n",
       "      <th>BASAB</th>\n",
       "      <th>ALB</th>\n",
       "      <th>ALP</th>\n",
       "      <th>BILI</th>\n",
       "      <th>no_sample_series</th>\n",
       "      <th>sex</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>age_at_admin</th>\n",
       "      <th>Comorb_score</th>\n",
       "      <th>Spcfc_Comorb</th>\n",
       "      <th>Mortality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30454500</td>\n",
       "      <td>16.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>143.000</td>\n",
       "      <td>36.400</td>\n",
       "      <td>81.000</td>\n",
       "      <td>49.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>76.000</td>\n",
       "      <td>17.350</td>\n",
       "      <td>4.300</td>\n",
       "      <td>69.000</td>\n",
       "      <td>6.500</td>\n",
       "      <td>201.500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>81.000</td>\n",
       "      <td>3.130</td>\n",
       "      <td>25.800</td>\n",
       "      <td>80.500</td>\n",
       "      <td>5.525</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.020</td>\n",
       "      <td>30.000</td>\n",
       "      <td>120.500</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30454500</td>\n",
       "      <td>15.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>115.667</td>\n",
       "      <td>36.500</td>\n",
       "      <td>88.667</td>\n",
       "      <td>62.333</td>\n",
       "      <td>92.667</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>76.000</td>\n",
       "      <td>17.350</td>\n",
       "      <td>4.300</td>\n",
       "      <td>69.000</td>\n",
       "      <td>6.500</td>\n",
       "      <td>201.500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>81.000</td>\n",
       "      <td>3.130</td>\n",
       "      <td>25.800</td>\n",
       "      <td>80.500</td>\n",
       "      <td>5.525</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.020</td>\n",
       "      <td>30.000</td>\n",
       "      <td>120.500</td>\n",
       "      <td>8.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30454500</td>\n",
       "      <td>14.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>88.333</td>\n",
       "      <td>36.600</td>\n",
       "      <td>96.333</td>\n",
       "      <td>75.667</td>\n",
       "      <td>95.333</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>76.000</td>\n",
       "      <td>17.350</td>\n",
       "      <td>4.300</td>\n",
       "      <td>69.000</td>\n",
       "      <td>6.500</td>\n",
       "      <td>201.500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>81.000</td>\n",
       "      <td>3.130</td>\n",
       "      <td>25.800</td>\n",
       "      <td>80.500</td>\n",
       "      <td>5.525</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.020</td>\n",
       "      <td>30.000</td>\n",
       "      <td>120.500</td>\n",
       "      <td>8.000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30454500</td>\n",
       "      <td>13.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>61.000</td>\n",
       "      <td>36.700</td>\n",
       "      <td>104.000</td>\n",
       "      <td>89.000</td>\n",
       "      <td>98.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>76.000</td>\n",
       "      <td>17.350</td>\n",
       "      <td>4.300</td>\n",
       "      <td>69.000</td>\n",
       "      <td>6.500</td>\n",
       "      <td>201.500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>81.000</td>\n",
       "      <td>3.130</td>\n",
       "      <td>25.800</td>\n",
       "      <td>80.500</td>\n",
       "      <td>5.525</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.020</td>\n",
       "      <td>30.000</td>\n",
       "      <td>120.500</td>\n",
       "      <td>8.000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  admission_id     rr   ews  heart_rate  temperature     sbp    dbp  \\\n",
       "0     30454500 16.000 9.000     143.000       36.400  81.000 49.000   \n",
       "1     30454500 15.000 7.000     115.667       36.500  88.667 62.333   \n",
       "2     30454500 14.000 5.000      88.333       36.600  96.333 75.667   \n",
       "3     30454500 13.000 3.000      61.000       36.700 104.000 89.000   \n",
       "\n",
       "   Oxygen_Saturation  Assisted_O2  Confusion   CREA   UREA     K    GFR   WBC  \\\n",
       "0             90.000        1.000      1.000 76.000 17.350 4.300 69.000 6.500   \n",
       "1             92.667        1.000      1.000 76.000 17.350 4.300 69.000 6.500   \n",
       "2             95.333        1.000      1.000 76.000 17.350 4.300 69.000 6.500   \n",
       "3             98.000        1.000      0.000 76.000 17.350 4.300 69.000 6.500   \n",
       "\n",
       "      PLT   HCT    HGB   RBC    MCH    MCV  NEUAB  TLYMAB  EOSAB  MONAB  \\\n",
       "0 201.500 0.250 81.000 3.130 25.800 80.500  5.525   0.640  0.025  0.270   \n",
       "1 201.500 0.250 81.000 3.130 25.800 80.500  5.525   0.640  0.025  0.270   \n",
       "2 201.500 0.250 81.000 3.130 25.800 80.500  5.525   0.640  0.025  0.270   \n",
       "3 201.500 0.250 81.000 3.130 25.800 80.500  5.525   0.640  0.025  0.270   \n",
       "\n",
       "   BASAB    ALB     ALP  BILI no_sample_series   sex  ethnicity  age_at_admin  \\\n",
       "0  0.020 30.000 120.500 8.000                0 1.000      0.000        72.000   \n",
       "1  0.020 30.000 120.500 8.000                1 1.000      0.000        72.000   \n",
       "2  0.020 30.000 120.500 8.000                2 1.000      0.000        72.000   \n",
       "3  0.020 30.000 120.500 8.000                3 1.000      0.000        72.000   \n",
       "\n",
       "   Comorb_score  Spcfc_Comorb  Mortality  \n",
       "0         0.000         1.000      0.000  \n",
       "1         0.000         1.000      0.000  \n",
       "2         0.000         1.000      0.000  \n",
       "3         0.000         1.000      0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_data = pd.concat([X_data_16_18, X_data_19_20])\n",
    "display(X_data.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feat_list = X_data.columns.tolist()\n",
    "feat_list = feat_list[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_data['admission_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning Hyperparameters\n",
    "- optimizer\n",
    "- learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import floor\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Flatten\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "trainSet = pd.read_csv('train.csv')\n",
    "# Feature generation: training data\n",
    "train = trainSet.drop(columns=['Name', 'Ticket', 'Cabin'])\n",
    "train = train.dropna(axis=0)\n",
    "train = pd.get_dummies(train)\n",
    "# train validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train.drop(columns=['PassengerId','Survived'], axis=0),\n",
    "                                                  train['Survived'], test_size=0.2, random_state=111, \n",
    "                                                  stratify=train['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def nn_cl_bo(neurons, activation, optimizer, learning_rate,  batch_size, epochs ):\n",
    "    optimizerD = {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate), 'RMSprop':RMSprop(lr=learning_rate),\n",
    "                  'Adadelta':Adadelta(lr=learning_rate),'Adagrad':Adagrad(lr=learning_rate), \n",
    "                  'Adamax':Adamax(lr=learning_rate),'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    neurons    = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs     = round(epochs)\n",
    "    opt        = list(optimizerD.keys())[round(optimizer)]\n",
    "    \n",
    "    # Make scorer accuracy\n",
    "    score_acc = make_scorer(accuracy_score)\n",
    "    \n",
    "    def nn_cl_fun():        \n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(neurons, input_dim=10, activation=activation))\n",
    "        nn.add(Dense(neurons, activation=activation))\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return nn\n",
    "    \n",
    "    es    = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn    = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  epochs   | learni... |  neurons  | optimizer |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6983  \u001b[0m | \u001b[0m 5.51    \u001b[0m | \u001b[0m 335.3   \u001b[0m | \u001b[0m 54.88   \u001b[0m | \u001b[0m 0.7716  \u001b[0m | \u001b[0m 36.58   \u001b[0m | \u001b[0m 1.044   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.7629  \u001b[0m | \u001b[95m 0.2023  \u001b[0m | \u001b[95m 536.2   \u001b[0m | \u001b[95m 39.09   \u001b[0m | \u001b[95m 0.3443  \u001b[0m | \u001b[95m 99.16   \u001b[0m | \u001b[95m 1.664   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.6186  \u001b[0m | \u001b[0m 0.7307  \u001b[0m | \u001b[0m 735.7   \u001b[0m | \u001b[0m 69.7    \u001b[0m | \u001b[0m 0.2815  \u001b[0m | \u001b[0m 51.96   \u001b[0m | \u001b[0m 0.8286  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5719  \u001b[0m | \u001b[0m 0.6656  \u001b[0m | \u001b[0m 920.6   \u001b[0m | \u001b[0m 83.52   \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 83.37   \u001b[0m | \u001b[0m 6.937   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7385  \u001b[0m | \u001b[0m 5.195   \u001b[0m | \u001b[0m 851.0   \u001b[0m | \u001b[0m 53.71   \u001b[0m | \u001b[0m 0.03717 \u001b[0m | \u001b[0m 50.87   \u001b[0m | \u001b[0m 0.7373  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5719  \u001b[0m | \u001b[0m 7.355   \u001b[0m | \u001b[0m 758.2   \u001b[0m | \u001b[0m 65.22   \u001b[0m | \u001b[0m 0.2815  \u001b[0m | \u001b[0m 99.86   \u001b[0m | \u001b[0m 0.9663  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5488  \u001b[0m | \u001b[0m 5.539   \u001b[0m | \u001b[0m 588.0   \u001b[0m | \u001b[0m 52.4    \u001b[0m | \u001b[0m 0.7306  \u001b[0m | \u001b[0m 39.05   \u001b[0m | \u001b[0m 2.804   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5719  \u001b[0m | \u001b[0m 2.871   \u001b[0m | \u001b[0m 957.8   \u001b[0m | \u001b[0m 93.5    \u001b[0m | \u001b[0m 0.8157  \u001b[0m | \u001b[0m 13.07   \u001b[0m | \u001b[0m 6.604   \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.7664  \u001b[0m | \u001b[95m 8.554   \u001b[0m | \u001b[95m 845.3   \u001b[0m | \u001b[95m 58.5    \u001b[0m | \u001b[95m 0.9671  \u001b[0m | \u001b[95m 47.53   \u001b[0m | \u001b[95m 2.232   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.766   \u001b[0m | \u001b[0m 0.148   \u001b[0m | \u001b[0m 230.5   \u001b[0m | \u001b[0m 24.25   \u001b[0m | \u001b[0m 0.1367  \u001b[0m | \u001b[0m 13.0    \u001b[0m | \u001b[0m 1.585   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6613  \u001b[0m | \u001b[0m 4.895   \u001b[0m | \u001b[0m 342.9   \u001b[0m | \u001b[0m 34.35   \u001b[0m | \u001b[0m 0.1581  \u001b[0m | \u001b[0m 71.47   \u001b[0m | \u001b[0m 3.283   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.5719  \u001b[0m | \u001b[0m 6.914   \u001b[0m | \u001b[0m 735.1   \u001b[0m | \u001b[0m 55.3    \u001b[0m | \u001b[0m 0.5993  \u001b[0m | \u001b[0m 51.55   \u001b[0m | \u001b[0m 6.743   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6234  \u001b[0m | \u001b[0m 1.33    \u001b[0m | \u001b[0m 925.5   \u001b[0m | \u001b[0m 59.83   \u001b[0m | \u001b[0m 0.5966  \u001b[0m | \u001b[0m 71.62   \u001b[0m | \u001b[0m 1.242   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.546   \u001b[0m | \u001b[0m 7.782   \u001b[0m | \u001b[0m 585.7   \u001b[0m | \u001b[0m 25.55   \u001b[0m | \u001b[0m 0.3711  \u001b[0m | \u001b[0m 42.54   \u001b[0m | \u001b[0m 3.304   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7652  \u001b[0m | \u001b[0m 1.615   \u001b[0m | \u001b[0m 340.2   \u001b[0m | \u001b[0m 95.93   \u001b[0m | \u001b[0m 0.6591  \u001b[0m | \u001b[0m 22.15   \u001b[0m | \u001b[0m 6.495   \u001b[0m |\n",
      "| \u001b[95m 16      \u001b[0m | \u001b[95m 0.7683  \u001b[0m | \u001b[95m 7.576   \u001b[0m | \u001b[95m 242.2   \u001b[0m | \u001b[95m 36.29   \u001b[0m | \u001b[95m 0.8738  \u001b[0m | \u001b[95m 70.65   \u001b[0m | \u001b[95m 2.081   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.5719  \u001b[0m | \u001b[0m 6.61    \u001b[0m | \u001b[0m 694.7   \u001b[0m | \u001b[0m 36.84   \u001b[0m | \u001b[0m 0.804   \u001b[0m | \u001b[0m 15.32   \u001b[0m | \u001b[0m 2.158   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.6296  \u001b[0m | \u001b[0m 1.866   \u001b[0m | \u001b[0m 977.8   \u001b[0m | \u001b[0m 92.75   \u001b[0m | \u001b[0m 0.6797  \u001b[0m | \u001b[0m 20.37   \u001b[0m | \u001b[0m 6.706   \u001b[0m |\n",
      "| \u001b[95m 19      \u001b[0m | \u001b[95m 0.7698  \u001b[0m | \u001b[95m 0.8254  \u001b[0m | \u001b[95m 703.8   \u001b[0m | \u001b[95m 92.23   \u001b[0m | \u001b[95m 0.3464  \u001b[0m | \u001b[95m 68.75   \u001b[0m | \u001b[95m 6.476   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.5758  \u001b[0m | \u001b[0m 3.366   \u001b[0m | \u001b[0m 817.1   \u001b[0m | \u001b[0m 91.69   \u001b[0m | \u001b[0m 0.624   \u001b[0m | \u001b[0m 23.6    \u001b[0m | \u001b[0m 2.624   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.6369  \u001b[0m | \u001b[0m 5.723   \u001b[0m | \u001b[0m 567.3   \u001b[0m | \u001b[0m 62.58   \u001b[0m | \u001b[0m 0.3588  \u001b[0m | \u001b[0m 69.39   \u001b[0m | \u001b[0m 3.336   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.6183  \u001b[0m | \u001b[0m 4.091   \u001b[0m | \u001b[0m 299.8   \u001b[0m | \u001b[0m 53.0    \u001b[0m | \u001b[0m 0.2804  \u001b[0m | \u001b[0m 41.21   \u001b[0m | \u001b[0m 6.821   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.6253  \u001b[0m | \u001b[0m 1.94    \u001b[0m | \u001b[0m 746.3   \u001b[0m | \u001b[0m 22.54   \u001b[0m | \u001b[0m 0.837   \u001b[0m | \u001b[0m 73.15   \u001b[0m | \u001b[0m 6.762   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.7681  \u001b[0m | \u001b[0m 5.326   \u001b[0m | \u001b[0m 373.9   \u001b[0m | \u001b[0m 77.54   \u001b[0m | \u001b[0m 0.04056 \u001b[0m | \u001b[0m 47.68   \u001b[0m | \u001b[0m 1.969   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.7681  \u001b[0m | \u001b[0m 0.9562  \u001b[0m | \u001b[0m 541.1   \u001b[0m | \u001b[0m 87.25   \u001b[0m | \u001b[0m 0.1193  \u001b[0m | \u001b[0m 98.8    \u001b[0m | \u001b[0m 1.633   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.7687  \u001b[0m | \u001b[0m 4.5     \u001b[0m | \u001b[0m 209.4   \u001b[0m | \u001b[0m 23.08   \u001b[0m | \u001b[0m 0.9636  \u001b[0m | \u001b[0m 49.95   \u001b[0m | \u001b[0m 0.2078  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.7651  \u001b[0m | \u001b[0m 8.422   \u001b[0m | \u001b[0m 498.0   \u001b[0m | \u001b[0m 68.27   \u001b[0m | \u001b[0m 0.8821  \u001b[0m | \u001b[0m 99.19   \u001b[0m | \u001b[0m 4.539   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.7654  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 385.0   \u001b[0m | \u001b[0m 98.87   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.7697  \u001b[0m | \u001b[0m 1.023   \u001b[0m | \u001b[0m 210.2   \u001b[0m | \u001b[0m 71.2    \u001b[0m | \u001b[0m 0.8684  \u001b[0m | \u001b[0m 41.4    \u001b[0m | \u001b[0m 1.665   \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Set paramaters\n",
    "params_nn ={\n",
    "    'neurons': (10, 100),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(200, 1000),\n",
    "    'epochs':(20, 100)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=111)\n",
    "nn_bo.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'sigmoid',\n",
       " 'batch_size': 704,\n",
       " 'epochs': 92,\n",
       " 'learning_rate': 0.3463993048863762,\n",
       " 'neurons': 69,\n",
       " 'optimizer': 'Nadam'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_nn_  = nn_bo.max['params']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu','elu', 'exponential', LeakyReLU,'relu']\n",
    "optimizerL  = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
    "\n",
    "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
    "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
    "params_nn_['epochs']     = round(params_nn_['epochs'])\n",
    "params_nn_['neurons']    = round(params_nn_['neurons'])\n",
    "params_nn_['optimizer']  = optimizerL[round(params_nn_['optimizer'])]\n",
    "params_nn_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning with Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create function\n",
    "def nn_cl_bo2(neurons, activation, optimizer, learning_rate, batch_size, epochs,layers1, layers2, normalization, dropout, dropout_rate):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    optimizerD = {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    neurons = round(neurons)\n",
    "    activation1 = activationL[round(activation1)]\n",
    "    activation2 = activationL[round(activation2)]\n",
    "    optimizer  = list(optimizerD.keys())[round(optimizer)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs     = round(epochs)\n",
    "    layers1    = round(layers1)\n",
    "    layers2    = round(layers2)\n",
    "    \n",
    "    def nn_cl_fun():\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(neurons, input_dim=10, activation=activation1))\n",
    "        if normalization > 0.5:\n",
    "            nn.add(BatchNormalization())\n",
    "        for i in range(layers1):\n",
    "            nn.add(Dense(neurons, activation=activation1))\n",
    "        if dropout > 0.5:\n",
    "            nn.add(Dropout(dropout_rate, seed=123))\n",
    "        for i in range(layers2):\n",
    "            nn.add(Dense(neurons, activation=activation2))\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  dropout  | dropou... |  epochs   |  layers1  |  layers2  | learni... |  neurons  | normal... | optimizer |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6416  \u001b[0m | \u001b[0m 5.51    \u001b[0m | \u001b[0m 335.3   \u001b[0m | \u001b[0m 0.4361  \u001b[0m | \u001b[0m 0.2308  \u001b[0m | \u001b[0m 43.63   \u001b[0m | \u001b[0m 1.298   \u001b[0m | \u001b[0m 1.045   \u001b[0m | \u001b[0m 0.426   \u001b[0m | \u001b[0m 31.48   \u001b[0m | \u001b[0m 0.3377  \u001b[0m | \u001b[0m 6.935   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.769   \u001b[0m | \u001b[95m 2.14    \u001b[0m | \u001b[95m 265.0   \u001b[0m | \u001b[95m 0.6696  \u001b[0m | \u001b[95m 0.1864  \u001b[0m | \u001b[95m 41.94   \u001b[0m | \u001b[95m 1.932   \u001b[0m | \u001b[95m 1.237   \u001b[0m | \u001b[95m 0.08322 \u001b[0m | \u001b[95m 91.07   \u001b[0m | \u001b[95m 0.794   \u001b[0m | \u001b[95m 5.884   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5719  \u001b[0m | \u001b[0m 7.337   \u001b[0m | \u001b[0m 992.8   \u001b[0m | \u001b[0m 0.5773  \u001b[0m | \u001b[0m 0.2441  \u001b[0m | \u001b[0m 53.71   \u001b[0m | \u001b[0m 1.055   \u001b[0m | \u001b[0m 1.908   \u001b[0m | \u001b[0m 0.1143  \u001b[0m | \u001b[0m 83.55   \u001b[0m | \u001b[0m 0.6977  \u001b[0m | \u001b[0m 3.957   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5792  \u001b[0m | \u001b[0m 2.468   \u001b[0m | \u001b[0m 998.8   \u001b[0m | \u001b[0m 0.138   \u001b[0m | \u001b[0m 0.1846  \u001b[0m | \u001b[0m 58.8    \u001b[0m | \u001b[0m 1.81    \u001b[0m | \u001b[0m 2.456   \u001b[0m | \u001b[0m 0.3296  \u001b[0m | \u001b[0m 46.05   \u001b[0m | \u001b[0m 0.319   \u001b[0m | \u001b[0m 6.631   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.769   \u001b[0m | \u001b[95m 8.268   \u001b[0m | \u001b[95m 851.1   \u001b[0m | \u001b[95m 0.03408 \u001b[0m | \u001b[95m 0.283   \u001b[0m | \u001b[95m 96.04   \u001b[0m | \u001b[95m 2.613   \u001b[0m | \u001b[95m 1.963   \u001b[0m | \u001b[95m 0.9671  \u001b[0m | \u001b[95m 47.53   \u001b[0m | \u001b[95m 0.3188  \u001b[0m | \u001b[95m 0.1151  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.6133  \u001b[0m | \u001b[0m 0.3436  \u001b[0m | \u001b[0m 242.5   \u001b[0m | \u001b[0m 0.128   \u001b[0m | \u001b[0m 0.01001 \u001b[0m | \u001b[0m 38.11   \u001b[0m | \u001b[0m 2.088   \u001b[0m | \u001b[0m 1.357   \u001b[0m | \u001b[0m 0.1876  \u001b[0m | \u001b[0m 23.47   \u001b[0m | \u001b[0m 0.683   \u001b[0m | \u001b[0m 3.283   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5719  \u001b[0m | \u001b[0m 6.914   \u001b[0m | \u001b[0m 735.1   \u001b[0m | \u001b[0m 0.4413  \u001b[0m | \u001b[0m 0.1786  \u001b[0m | \u001b[0m 56.93   \u001b[0m | \u001b[0m 2.927   \u001b[0m | \u001b[0m 1.296   \u001b[0m | \u001b[0m 0.9077  \u001b[0m | \u001b[0m 54.81   \u001b[0m | \u001b[0m 0.5925  \u001b[0m | \u001b[0m 4.793   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.7694  \u001b[0m | \u001b[95m 1.597   \u001b[0m | \u001b[95m 891.7   \u001b[0m | \u001b[95m 0.4821  \u001b[0m | \u001b[95m 0.0208  \u001b[0m | \u001b[95m 49.18   \u001b[0m | \u001b[95m 1.723   \u001b[0m | \u001b[95m 1.944   \u001b[0m | \u001b[95m 0.1877  \u001b[0m | \u001b[95m 25.78   \u001b[0m | \u001b[95m 0.9491  \u001b[0m | \u001b[95m 4.59    \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5719  \u001b[0m | \u001b[0m 1.215   \u001b[0m | \u001b[0m 942.2   \u001b[0m | \u001b[0m 0.8418  \u001b[0m | \u001b[0m 0.01583 \u001b[0m | \u001b[0m 36.29   \u001b[0m | \u001b[0m 2.745   \u001b[0m | \u001b[0m 2.348   \u001b[0m | \u001b[0m 0.3043  \u001b[0m | \u001b[0m 76.1    \u001b[0m | \u001b[0m 0.6183  \u001b[0m | \u001b[0m 1.473   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5719  \u001b[0m | \u001b[0m 7.219   \u001b[0m | \u001b[0m 247.3   \u001b[0m | \u001b[0m 0.3082  \u001b[0m | \u001b[0m 0.06221 \u001b[0m | \u001b[0m 97.78   \u001b[0m | \u001b[0m 2.819   \u001b[0m | \u001b[0m 2.353   \u001b[0m | \u001b[0m 0.124   \u001b[0m | \u001b[0m 96.22   \u001b[0m | \u001b[0m 0.09171 \u001b[0m | \u001b[0m 4.409   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6655  \u001b[0m | \u001b[0m 8.126   \u001b[0m | \u001b[0m 471.8   \u001b[0m | \u001b[0m 0.6528  \u001b[0m | \u001b[0m 0.2775  \u001b[0m | \u001b[0m 49.92   \u001b[0m | \u001b[0m 2.543   \u001b[0m | \u001b[0m 2.792   \u001b[0m | \u001b[0m 0.624   \u001b[0m | \u001b[0m 23.6    \u001b[0m | \u001b[0m 0.3749  \u001b[0m | \u001b[0m 4.451   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.5719  \u001b[0m | \u001b[0m 4.132   \u001b[0m | \u001b[0m 625.8   \u001b[0m | \u001b[0m 0.3523  \u001b[0m | \u001b[0m 0.198   \u001b[0m | \u001b[0m 58.12   \u001b[0m | \u001b[0m 1.909   \u001b[0m | \u001b[0m 1.25    \u001b[0m | \u001b[0m 0.4183  \u001b[0m | \u001b[0m 34.58   \u001b[0m | \u001b[0m 0.3467  \u001b[0m | \u001b[0m 6.821   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.6195  \u001b[0m | \u001b[0m 1.94    \u001b[0m | \u001b[0m 746.3   \u001b[0m | \u001b[0m 0.03181 \u001b[0m | \u001b[0m 0.2506  \u001b[0m | \u001b[0m 76.13   \u001b[0m | \u001b[0m 2.932   \u001b[0m | \u001b[0m 2.184   \u001b[0m | \u001b[0m 0.2252  \u001b[0m | \u001b[0m 74.73   \u001b[0m | \u001b[0m 0.03087 \u001b[0m | \u001b[0m 2.931   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7679  \u001b[0m | \u001b[0m 2.531   \u001b[0m | \u001b[0m 285.0   \u001b[0m | \u001b[0m 0.4263  \u001b[0m | \u001b[0m 0.2522  \u001b[0m | \u001b[0m 28.83   \u001b[0m | \u001b[0m 2.973   \u001b[0m | \u001b[0m 1.467   \u001b[0m | \u001b[0m 0.7242  \u001b[0m | \u001b[0m 69.48   \u001b[0m | \u001b[0m 0.07776 \u001b[0m | \u001b[0m 4.881   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7649  \u001b[0m | \u001b[0m 2.388   \u001b[0m | \u001b[0m 921.5   \u001b[0m | \u001b[0m 0.8183  \u001b[0m | \u001b[0m 0.1198  \u001b[0m | \u001b[0m 85.62   \u001b[0m | \u001b[0m 1.396   \u001b[0m | \u001b[0m 2.045   \u001b[0m | \u001b[0m 0.4184  \u001b[0m | \u001b[0m 93.33   \u001b[0m | \u001b[0m 0.8254  \u001b[0m | \u001b[0m 3.507   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.5719  \u001b[0m | \u001b[0m 1.051   \u001b[0m | \u001b[0m 209.3   \u001b[0m | \u001b[0m 0.9132  \u001b[0m | \u001b[0m 0.1537  \u001b[0m | \u001b[0m 87.45   \u001b[0m | \u001b[0m 1.19    \u001b[0m | \u001b[0m 2.607   \u001b[0m | \u001b[0m 0.07161 \u001b[0m | \u001b[0m 67.19   \u001b[0m | \u001b[0m 0.9688  \u001b[0m | \u001b[0m 2.782   \u001b[0m |\n",
      "| \u001b[95m 17      \u001b[0m | \u001b[95m 0.7697  \u001b[0m | \u001b[95m 5.936   \u001b[0m | \u001b[95m 371.9   \u001b[0m | \u001b[95m 0.8899  \u001b[0m | \u001b[95m 0.296   \u001b[0m | \u001b[95m 79.09   \u001b[0m | \u001b[95m 2.283   \u001b[0m | \u001b[95m 1.504   \u001b[0m | \u001b[95m 0.4811  \u001b[0m | \u001b[95m 34.13   \u001b[0m | \u001b[95m 0.8683  \u001b[0m | \u001b[95m 1.868   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.7642  \u001b[0m | \u001b[0m 8.757   \u001b[0m | \u001b[0m 370.8   \u001b[0m | \u001b[0m 0.2978  \u001b[0m | \u001b[0m 0.221   \u001b[0m | \u001b[0m 21.03   \u001b[0m | \u001b[0m 1.06    \u001b[0m | \u001b[0m 2.468   \u001b[0m | \u001b[0m 0.5033  \u001b[0m | \u001b[0m 29.63   \u001b[0m | \u001b[0m 0.00893 \u001b[0m | \u001b[0m 5.955   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.6487  \u001b[0m | \u001b[0m 4.828   \u001b[0m | \u001b[0m 778.8   \u001b[0m | \u001b[0m 0.6616  \u001b[0m | \u001b[0m 0.2516  \u001b[0m | \u001b[0m 51.06   \u001b[0m | \u001b[0m 1.852   \u001b[0m | \u001b[0m 2.656   \u001b[0m | \u001b[0m 0.4743  \u001b[0m | \u001b[0m 83.8    \u001b[0m | \u001b[0m 0.01418 \u001b[0m | \u001b[0m 2.777   \u001b[0m |\n",
      "| \u001b[95m 20      \u001b[0m | \u001b[95m 0.7699  \u001b[0m | \u001b[95m 1.155   \u001b[0m | \u001b[95m 294.5   \u001b[0m | \u001b[95m 0.206   \u001b[0m | \u001b[95m 0.2243  \u001b[0m | \u001b[95m 94.41   \u001b[0m | \u001b[95m 1.761   \u001b[0m | \u001b[95m 1.921   \u001b[0m | \u001b[95m 0.8746  \u001b[0m | \u001b[95m 83.31   \u001b[0m | \u001b[95m 0.02497 \u001b[0m | \u001b[95m 6.111   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.7674  \u001b[0m | \u001b[0m 5.441   \u001b[0m | \u001b[0m 613.2   \u001b[0m | \u001b[0m 0.5893  \u001b[0m | \u001b[0m 0.2399  \u001b[0m | \u001b[0m 33.86   \u001b[0m | \u001b[0m 1.374   \u001b[0m | \u001b[0m 1.516   \u001b[0m | \u001b[0m 0.06056 \u001b[0m | \u001b[0m 59.74   \u001b[0m | \u001b[0m 0.3518  \u001b[0m | \u001b[0m 6.419   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.5975  \u001b[0m | \u001b[0m 4.289   \u001b[0m | \u001b[0m 283.6   \u001b[0m | \u001b[0m 0.1525  \u001b[0m | \u001b[0m 0.08206 \u001b[0m | \u001b[0m 82.52   \u001b[0m | \u001b[0m 1.786   \u001b[0m | \u001b[0m 2.598   \u001b[0m | \u001b[0m 0.4387  \u001b[0m | \u001b[0m 17.34   \u001b[0m | \u001b[0m 0.01064 \u001b[0m | \u001b[0m 3.016   \u001b[0m |\n",
      "| \u001b[95m 23      \u001b[0m | \u001b[95m 0.77    \u001b[0m | \u001b[95m 5.966   \u001b[0m | \u001b[95m 612.2   \u001b[0m | \u001b[95m 0.5801  \u001b[0m | \u001b[95m 0.1479  \u001b[0m | \u001b[95m 79.24   \u001b[0m | \u001b[95m 2.579   \u001b[0m | \u001b[95m 2.562   \u001b[0m | \u001b[95m 0.1363  \u001b[0m | \u001b[95m 94.61   \u001b[0m | \u001b[95m 0.8777  \u001b[0m | \u001b[95m 4.897   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.7547  \u001b[0m | \u001b[0m 8.432   \u001b[0m | \u001b[0m 739.0   \u001b[0m | \u001b[0m 0.5944  \u001b[0m | \u001b[0m 0.1035  \u001b[0m | \u001b[0m 26.69   \u001b[0m | \u001b[0m 2.159   \u001b[0m | \u001b[0m 1.035   \u001b[0m | \u001b[0m 0.5569  \u001b[0m | \u001b[0m 66.93   \u001b[0m | \u001b[0m 0.6784  \u001b[0m | \u001b[0m 1.194   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.7684  \u001b[0m | \u001b[0m 5.194   \u001b[0m | \u001b[0m 364.8   \u001b[0m | \u001b[0m 0.2515  \u001b[0m | \u001b[0m 0.2908  \u001b[0m | \u001b[0m 91.73   \u001b[0m | \u001b[0m 1.246   \u001b[0m | \u001b[0m 2.762   \u001b[0m | \u001b[0m 0.9485  \u001b[0m | \u001b[0m 51.39   \u001b[0m | \u001b[0m 0.413   \u001b[0m | \u001b[0m 4.04    \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.7698  \u001b[0m | \u001b[0m 0.3364  \u001b[0m | \u001b[0m 295.9   \u001b[0m | \u001b[0m 0.4407  \u001b[0m | \u001b[0m 0.2814  \u001b[0m | \u001b[0m 57.46   \u001b[0m | \u001b[0m 2.107   \u001b[0m | \u001b[0m 1.592   \u001b[0m | \u001b[0m 0.8504  \u001b[0m | \u001b[0m 87.88   \u001b[0m | \u001b[0m 0.2735  \u001b[0m | \u001b[0m 6.238   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.763   \u001b[0m | \u001b[0m 8.265   \u001b[0m | \u001b[0m 369.0   \u001b[0m | \u001b[0m 0.9605  \u001b[0m | \u001b[0m 0.04568 \u001b[0m | \u001b[0m 21.83   \u001b[0m | \u001b[0m 1.493   \u001b[0m | \u001b[0m 1.521   \u001b[0m | \u001b[0m 0.2533  \u001b[0m | \u001b[0m 26.23   \u001b[0m | \u001b[0m 0.01586 \u001b[0m | \u001b[0m 6.263   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.5719  \u001b[0m | \u001b[0m 6.51    \u001b[0m | \u001b[0m 601.9   \u001b[0m | \u001b[0m 0.5418  \u001b[0m | \u001b[0m 0.2301  \u001b[0m | \u001b[0m 34.42   \u001b[0m | \u001b[0m 1.513   \u001b[0m | \u001b[0m 2.219   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 96.43   \u001b[0m | \u001b[0m 0.5967  \u001b[0m | \u001b[0m 5.069   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.6781  \u001b[0m | \u001b[0m 4.662   \u001b[0m | \u001b[0m 875.4   \u001b[0m | \u001b[0m 0.308   \u001b[0m | \u001b[0m 0.1438  \u001b[0m | \u001b[0m 74.24   \u001b[0m | \u001b[0m 2.042   \u001b[0m | \u001b[0m 1.962   \u001b[0m | \u001b[0m 0.5502  \u001b[0m | \u001b[0m 40.74   \u001b[0m | \u001b[0m 0.6807  \u001b[0m | \u001b[0m 2.602   \u001b[0m |\n",
      "=============================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "params_nn2 ={\n",
    "    'neurons': (10, 100),\n",
    "    'activation1':(0, 9),\n",
    "    'activation2':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(200, 1000),\n",
    "    'epochs':(20, 100),\n",
    "    'layers1':(1,3),\n",
    "    'layers2':(1,3),\n",
    "    'normalization':(0,1),\n",
    "    'dropout':(0,1),\n",
    "    'dropout_rate':(0,0.3)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)\n",
    "nn_bo.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'elu',\n",
       " 'batch_size': 612,\n",
       " 'dropout': 0.5801259080387516,\n",
       " 'dropout_rate': 0.14793085825348554,\n",
       " 'epochs': 79,\n",
       " 'layers1': 3,\n",
       " 'layers2': 3,\n",
       " 'learning_rate': 0.13626453282900686,\n",
       " 'neurons': 95,\n",
       " 'normalization': 0.8777167752038778,\n",
       " 'optimizer': 'Adamax'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_nn_ = nn_bo.max['params']\n",
    "learning_rate = params_nn_['learning_rate']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
    "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
    "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
    "params_nn_['layers1'] = round(params_nn_['layers1'])\n",
    "params_nn_['layers2'] = round(params_nn_['layers2'])\n",
    "params_nn_['neurons'] = round(params_nn_['neurons'])\n",
    "\n",
    "optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "             'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "             'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "             'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "params_nn_['optimizer'] = list(optimizerD.keys())[round(params_nn_['optimizer'])]\n",
    "params_nn_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7657, 144, 33) (7657,)\n"
     ]
    }
   ],
   "source": [
    "df_admissions = df_admissions_16_18.copy()\n",
    "X_data = X_data_16_18.copy()\n",
    "\n",
    "list_admins      = X_data['admission_id'].unique().tolist()\n",
    "list_admins_ord  = df_admissions[df_admissions['admission_id'].isin(list_admins)].sort_values(by = 'admission_date_time')['admission_id']\n",
    "\n",
    "num_train_admins = int(0.6 * len(list_admins_ord))\n",
    "num_val_admins   = int(0.2 * len(list_admins_ord))\n",
    "num_test_admins  = len(list_admins_ord) - num_train_admins - num_val_admins\n",
    "\n",
    "feat_list = X_data.columns.tolist()[1:-1]\n",
    "####################################################\n",
    "####################################################\n",
    "#  2. NORMALISATION OF TRAINING SET\n",
    "####################################################\n",
    "raw_data  = X_data[feat_list]\n",
    "mortality = X_data['Mortality'].tolist()\n",
    "\n",
    "\n",
    "mean = raw_data[:num_train_admins * 144].mean(axis=0)\n",
    "std  = raw_data[:num_train_admins * 144].std(axis=0)\n",
    "std  = [1 if s == 0 else s for s in std]\n",
    "\n",
    "norm_data = raw_data - mean\n",
    "norm_data /= std\n",
    "\n",
    "norm_data = norm_data.drop(columns = 'no_sample_series')\n",
    "\n",
    "num_samples  = len(X_data['admission_id'].unique().tolist())\n",
    "num_features = len(norm_data.columns.tolist())\n",
    "\n",
    "X_train_mio = np.array(norm_data).reshape((num_samples, 144, num_features ))\n",
    "\n",
    "\n",
    "\n",
    "df          = X_data[['admission_id', 'Mortality']].groupby(by= 'admission_id').mean()\n",
    "mort_dict   = dict(zip(df.index.tolist(),df['Mortality']))\n",
    "y_train_mio = np.array([mort_dict[adm] for adm in X_data['admission_id'].unique().tolist()])\n",
    "\n",
    "print(X_train_mio.shape, y_train_mio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7657\n",
      "33\n",
      "1102608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36386064"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(num_samples)\n",
    "print(num_features)\n",
    "print(len(norm_data))\n",
    "\n",
    "num_samples * num_features * 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "240/240 [==============================] - 1s 1ms/step - loss: 0.6313 - accuracy: 0.7584 - auc_1: 0.7001\n",
      "Epoch 2/4\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.4982 - accuracy: 0.7960 - auc_1: 0.7629\n",
      "Epoch 3/4\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.4549 - accuracy: 0.8037 - auc_1: 0.7829\n",
      "Epoch 4/4\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.4221 - accuracy: 0.8125 - auc_1: 0.8009\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nn = Sequential()\n",
    "nn.add(Flatten(input_shape=(144,33)))\n",
    "nn.add(Dense(16, activation=\"relu\"))\n",
    "nn.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "\n",
    "nn.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\", keras.metrics.AUC(from_logits=True)])\n",
    "history = nn.fit(X_train_mio, y_train_mio,\n",
    "                    epochs=4)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Adam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-31a9bce3d7ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepochs\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moptimizer\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.06\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdropout_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Adam' is not defined"
     ]
    }
   ],
   "source": [
    "neurons      = 3\n",
    "activation   = 'relu'\n",
    "batch_size   = 200\n",
    "epochs       = 30\n",
    "optimizer    = Adam(lr=0.06)\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# Make scorer accuracy\n",
    "score_acc = make_scorer(accuracy_score)\n",
    "\n",
    "        \n",
    "        \n",
    "def nn_cl_fun():        \n",
    "    nn = Sequenatial()\n",
    "    nn.add(Flatten(input_dim=(144,33)))\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dense(neurons1, activation=activation1))\n",
    "    nn.add(Dropout(dropout_rate1, seed=123))\n",
    "    nn.add(Dense(neurons2, activation=activation2))\n",
    "    nn.add(Dropout(dropout_rate2, seed=123))\n",
    "\n",
    "    nn.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "\n",
    "    nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return nn\n",
    "\n",
    "es    = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "nn    = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "score = cross_val_score(nn, X_train_mio, y_train_mio, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model in function with variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7642561986994626"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurons    = 3\n",
    "activation = 'relu'\n",
    "batch_size = 200\n",
    "epochs     = 30\n",
    "opt        = Adam(lr=0.06)\n",
    "\n",
    "# Make scorer accuracy\n",
    "score_acc = make_scorer(accuracy_score)\n",
    "\n",
    "def nn_cl_fun():        \n",
    "    inputs  = keras.Input(shape=(144,33))\n",
    "    x       = Flatten()(inputs)\n",
    "    x       = Dense(neurons, activation=activation)(x)\n",
    "    outputs = Dense(1, activation = activation)(x)\n",
    "    nn      = keras.Model(inputs, outputs)\n",
    "\n",
    "    nn.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return nn\n",
    "\n",
    "es    = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "nn    = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "score = cross_val_score(nn, X_train_mio, y_train_mio, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model in Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def nn_cl_bo(neurons, activation, optimizer, learning_rate,  batch_size, epochs ):\n",
    "    optimizerD = {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate), 'RMSprop':RMSprop(lr=learning_rate),\n",
    "                  'Adadelta':Adadelta(lr=learning_rate),'Adagrad':Adagrad(lr=learning_rate), \n",
    "                  'Adamax':Adamax(lr=learning_rate),'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    neurons    = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs     = round(epochs)\n",
    "    opt        = list(optimizerD.keys())[round(optimizer)]\n",
    "    \n",
    "    # Make scorer accuracy\n",
    "    score_acc = make_scorer(accuracy_score)\n",
    "    \n",
    "    def nn_cl_fun():        \n",
    "        inputs  = keras.Input(shape=(144,33))\n",
    "        x       = Flatten()(inputs)\n",
    "        x       = Dense(neurons, activation=activation)(x)        \n",
    "        x       = Dense(neurons, activation=activation)(x)\n",
    "        outputs = Dense(1, activation = activation)(x)\n",
    "        nn      = keras.Model(inputs, outputs)\n",
    "\n",
    "        nn.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "        return nn\n",
    "    \n",
    "    es    = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn    = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X_train_mio, y_train_mio, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  epochs   | learni... |  neurons  | optimizer |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8006  \u001b[0m | \u001b[0m 5.51    \u001b[0m | \u001b[0m 335.3   \u001b[0m | \u001b[0m 54.88   \u001b[0m | \u001b[0m 0.7716  \u001b[0m | \u001b[0m 36.58   \u001b[0m | \u001b[0m 1.044   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.795   \u001b[0m | \u001b[0m 0.2023  \u001b[0m | \u001b[0m 536.2   \u001b[0m | \u001b[0m 39.09   \u001b[0m | \u001b[0m 0.3443  \u001b[0m | \u001b[0m 99.16   \u001b[0m | \u001b[0m 1.664   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8179  \u001b[0m | \u001b[95m 0.7307  \u001b[0m | \u001b[95m 735.7   \u001b[0m | \u001b[95m 69.7    \u001b[0m | \u001b[95m 0.2815  \u001b[0m | \u001b[95m 51.96   \u001b[0m | \u001b[95m 0.8286  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8097  \u001b[0m | \u001b[0m 0.6656  \u001b[0m | \u001b[0m 920.6   \u001b[0m | \u001b[0m 83.52   \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 83.37   \u001b[0m | \u001b[0m 6.937   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7529  \u001b[0m | \u001b[0m 5.195   \u001b[0m | \u001b[0m 851.0   \u001b[0m | \u001b[0m 53.71   \u001b[0m | \u001b[0m 0.03717 \u001b[0m | \u001b[0m 50.87   \u001b[0m | \u001b[0m 0.7373  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8097  \u001b[0m | \u001b[0m 7.355   \u001b[0m | \u001b[0m 758.2   \u001b[0m | \u001b[0m 65.22   \u001b[0m | \u001b[0m 0.2815  \u001b[0m | \u001b[0m 99.86   \u001b[0m | \u001b[0m 0.9663  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.58    \u001b[0m | \u001b[0m 5.539   \u001b[0m | \u001b[0m 588.0   \u001b[0m | \u001b[0m 52.4    \u001b[0m | \u001b[0m 0.7306  \u001b[0m | \u001b[0m 39.05   \u001b[0m | \u001b[0m 2.804   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8097  \u001b[0m | \u001b[0m 2.871   \u001b[0m | \u001b[0m 957.8   \u001b[0m | \u001b[0m 93.5    \u001b[0m | \u001b[0m 0.8157  \u001b[0m | \u001b[0m 13.07   \u001b[0m | \u001b[0m 6.604   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7967  \u001b[0m | \u001b[0m 8.554   \u001b[0m | \u001b[0m 845.3   \u001b[0m | \u001b[0m 58.5    \u001b[0m | \u001b[0m 0.9671  \u001b[0m | \u001b[0m 47.53   \u001b[0m | \u001b[0m 2.232   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7948  \u001b[0m | \u001b[0m 0.148   \u001b[0m | \u001b[0m 230.5   \u001b[0m | \u001b[0m 24.25   \u001b[0m | \u001b[0m 0.1367  \u001b[0m | \u001b[0m 13.0    \u001b[0m | \u001b[0m 1.585   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.6049  \u001b[0m | \u001b[0m 4.895   \u001b[0m | \u001b[0m 342.9   \u001b[0m | \u001b[0m 34.35   \u001b[0m | \u001b[0m 0.1581  \u001b[0m | \u001b[0m 71.47   \u001b[0m | \u001b[0m 3.283   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.1903  \u001b[0m | \u001b[0m 6.914   \u001b[0m | \u001b[0m 735.1   \u001b[0m | \u001b[0m 55.3    \u001b[0m | \u001b[0m 0.5993  \u001b[0m | \u001b[0m 51.55   \u001b[0m | \u001b[0m 6.743   \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m 0.8204  \u001b[0m | \u001b[95m 1.33    \u001b[0m | \u001b[95m 925.5   \u001b[0m | \u001b[95m 59.83   \u001b[0m | \u001b[95m 0.5966  \u001b[0m | \u001b[95m 71.62   \u001b[0m | \u001b[95m 1.242   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.6185  \u001b[0m | \u001b[0m 7.782   \u001b[0m | \u001b[0m 585.7   \u001b[0m | \u001b[0m 25.55   \u001b[0m | \u001b[0m 0.3711  \u001b[0m | \u001b[0m 42.54   \u001b[0m | \u001b[0m 3.304   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7745  \u001b[0m | \u001b[0m 1.615   \u001b[0m | \u001b[0m 340.2   \u001b[0m | \u001b[0m 95.93   \u001b[0m | \u001b[0m 0.6591  \u001b[0m | \u001b[0m 22.15   \u001b[0m | \u001b[0m 6.495   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.7929  \u001b[0m | \u001b[0m 7.576   \u001b[0m | \u001b[0m 242.2   \u001b[0m | \u001b[0m 36.29   \u001b[0m | \u001b[0m 0.8738  \u001b[0m | \u001b[0m 70.65   \u001b[0m | \u001b[0m 2.081   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.8097  \u001b[0m | \u001b[0m 6.61    \u001b[0m | \u001b[0m 694.7   \u001b[0m | \u001b[0m 36.84   \u001b[0m | \u001b[0m 0.804   \u001b[0m | \u001b[0m 15.32   \u001b[0m | \u001b[0m 2.158   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.6413  \u001b[0m | \u001b[0m 1.866   \u001b[0m | \u001b[0m 977.8   \u001b[0m | \u001b[0m 92.75   \u001b[0m | \u001b[0m 0.6797  \u001b[0m | \u001b[0m 20.37   \u001b[0m | \u001b[0m 6.706   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.7999  \u001b[0m | \u001b[0m 0.8254  \u001b[0m | \u001b[0m 703.8   \u001b[0m | \u001b[0m 92.23   \u001b[0m | \u001b[0m 0.3464  \u001b[0m | \u001b[0m 68.75   \u001b[0m | \u001b[0m 6.476   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.7698  \u001b[0m | \u001b[0m 3.366   \u001b[0m | \u001b[0m 817.1   \u001b[0m | \u001b[0m 91.69   \u001b[0m | \u001b[0m 0.624   \u001b[0m | \u001b[0m 23.6    \u001b[0m | \u001b[0m 2.624   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.6387  \u001b[0m | \u001b[0m 5.723   \u001b[0m | \u001b[0m 567.3   \u001b[0m | \u001b[0m 62.58   \u001b[0m | \u001b[0m 0.3588  \u001b[0m | \u001b[0m 69.39   \u001b[0m | \u001b[0m 3.336   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.7778  \u001b[0m | \u001b[0m 4.091   \u001b[0m | \u001b[0m 299.8   \u001b[0m | \u001b[0m 53.0    \u001b[0m | \u001b[0m 0.2804  \u001b[0m | \u001b[0m 41.21   \u001b[0m | \u001b[0m 6.821   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.7918  \u001b[0m | \u001b[0m 1.94    \u001b[0m | \u001b[0m 746.3   \u001b[0m | \u001b[0m 22.54   \u001b[0m | \u001b[0m 0.837   \u001b[0m | \u001b[0m 73.15   \u001b[0m | \u001b[0m 6.762   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.8007  \u001b[0m | \u001b[0m 5.326   \u001b[0m | \u001b[0m 373.9   \u001b[0m | \u001b[0m 77.54   \u001b[0m | \u001b[0m 0.04056 \u001b[0m | \u001b[0m 47.68   \u001b[0m | \u001b[0m 1.969   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.797   \u001b[0m | \u001b[0m 0.9562  \u001b[0m | \u001b[0m 541.1   \u001b[0m | \u001b[0m 87.25   \u001b[0m | \u001b[0m 0.1193  \u001b[0m | \u001b[0m 98.8    \u001b[0m | \u001b[0m 1.633   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.7755  \u001b[0m | \u001b[0m 2.296   \u001b[0m | \u001b[0m 745.3   \u001b[0m | \u001b[0m 21.16   \u001b[0m | \u001b[0m 0.7684  \u001b[0m | \u001b[0m 73.69   \u001b[0m | \u001b[0m 5.39    \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.8097  \u001b[0m | \u001b[0m 1.166   \u001b[0m | \u001b[0m 928.2   \u001b[0m | \u001b[0m 66.85   \u001b[0m | \u001b[0m 0.01039 \u001b[0m | \u001b[0m 81.56   \u001b[0m | \u001b[0m 6.64    \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.5833  \u001b[0m | \u001b[0m 8.05    \u001b[0m | \u001b[0m 922.5   \u001b[0m | \u001b[0m 67.88   \u001b[0m | \u001b[0m 0.9407  \u001b[0m | \u001b[0m 76.3    \u001b[0m | \u001b[0m 3.228   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.7844  \u001b[0m | \u001b[0m 3.435   \u001b[0m | \u001b[0m 932.9   \u001b[0m | \u001b[0m 59.89   \u001b[0m | \u001b[0m 0.6777  \u001b[0m | \u001b[0m 73.6    \u001b[0m | \u001b[0m 3.349   \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Set paramaters\n",
    "params_nn ={\n",
    "    'neurons': (10, 100),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(200, 1000),\n",
    "    'epochs':(20, 100)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=111)\n",
    "nn_bo.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model optimization with layers optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def nn_cl_bo2(neurons1, neurons2, activation1, activation2, optimizer, learning_rate, batch_size, epochs,layers1, \n",
    "              layers2, normalization, dropout1, dropout_rate1, dropout2, dropout_rate2):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    optimizerD = {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    neurons1   = round(neurons1)\n",
    "    neurons2   = round(neurons2)\n",
    "    activation1 = activationL[round(activation1)]\n",
    "    activation2 = activationL[round(activation2)]\n",
    "    optimizer  = list(optimizerD.keys())[round(optimizer)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs     = round(epochs)\n",
    "    layers1    = round(layers1)\n",
    "    layers2    = round(layers2)\n",
    "    \n",
    "    score_acc = make_scorer(accuracy_score)\n",
    "    \n",
    "    def nn_cl_fun():        \n",
    "        inputs  = keras.Input(shape=(144,33))\n",
    "        x       = Flatten()(inputs)\n",
    "        x       = Dense(neurons1, activation=activation1)(x)\n",
    "        \n",
    "        if normalization > 0.5:\n",
    "            x = BatchNormalization()(x)     \n",
    "            \n",
    "        for i in range(layers1):\n",
    "            x = Dense(neurons1, activation=activation1)(x)\n",
    "        if dropout1 > 0.5:\n",
    "            x = Dropout(dropout_rate1, seed=123)(x)\n",
    "            \n",
    "        for i in range(layers2):\n",
    "            x = Dense(neurons2, activation=activation2)(x)\n",
    "        if dropout2 > 0.5:\n",
    "            x = Dropout(dropout_rate2, seed=123)(x)\n",
    "        \n",
    "        outputs = Dense(1, activation = activation2)(x)\n",
    "        nn      = keras.Model(inputs, outputs)\n",
    "        nn.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "        return nn\n",
    "\n",
    "        nn.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "        return nn\n",
    "    \n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X_train_mio, y_train_mio, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | activa... | batch_... | dropout1  | dropout2  | dropou... | dropou... |  epochs   |  layers1  |  layers2  | learni... | neurons1  | neurons2  | normal... | optimizer |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.568   \u001b[0m | \u001b[0m 5.51    \u001b[0m | \u001b[0m 1.522   \u001b[0m | \u001b[0m 548.8   \u001b[0m | \u001b[0m 0.7693  \u001b[0m | \u001b[0m 0.2953  \u001b[0m | \u001b[0m 0.1597  \u001b[0m | \u001b[0m 0.109   \u001b[0m | \u001b[0m 53.62   \u001b[0m | \u001b[0m 1.477   \u001b[0m | \u001b[0m 1.675   \u001b[0m | \u001b[0m 0.9908  \u001b[0m | \u001b[0m 31.4    \u001b[0m | \u001b[0m 17.31   \u001b[0m | \u001b[0m 0.6696  \u001b[0m | \u001b[0m 4.349   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.683   \u001b[0m | \u001b[95m 2.468   \u001b[0m | \u001b[95m 4.196   \u001b[0m | \u001b[95m 294.7   \u001b[0m | \u001b[95m 0.07396 \u001b[0m | \u001b[95m 0.9008  \u001b[0m | \u001b[95m 0.4176  \u001b[0m | \u001b[95m 0.4362  \u001b[0m | \u001b[95m 85.22   \u001b[0m | \u001b[95m 2.982   \u001b[0m | \u001b[95m 2.155   \u001b[0m | \u001b[95m 0.8156  \u001b[0m | \u001b[95m 47.92   \u001b[0m | \u001b[95m 12.47   \u001b[0m | \u001b[95m 0.4541  \u001b[0m | \u001b[95m 0.7373  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8097  \u001b[0m | \u001b[95m 7.355   \u001b[0m | \u001b[95m 6.28    \u001b[0m | \u001b[95m 652.2   \u001b[0m | \u001b[95m 0.2742  \u001b[0m | \u001b[95m 0.9985  \u001b[0m | \u001b[95m 0.1552  \u001b[0m | \u001b[95m 0.3462  \u001b[0m | \u001b[95m 58.8    \u001b[0m | \u001b[95m 1.81    \u001b[0m | \u001b[95m 2.456   \u001b[0m | \u001b[95m 0.3296  \u001b[0m | \u001b[95m 46.05   \u001b[0m | \u001b[95m 38.71   \u001b[0m | \u001b[95m 0.9472  \u001b[0m | \u001b[95m 6.431   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8097  \u001b[0m | \u001b[0m 7.325   \u001b[0m | \u001b[0m 0.3067  \u001b[0m | \u001b[0m 954.7   \u001b[0m | \u001b[0m 0.9504  \u001b[0m | \u001b[0m 0.8066  \u001b[0m | \u001b[0m 0.2925  \u001b[0m | \u001b[0m 0.4867  \u001b[0m | \u001b[0m 53.36   \u001b[0m | \u001b[0m 1.638   \u001b[0m | \u001b[0m 1.033   \u001b[0m | \u001b[0m 0.0478  \u001b[0m | \u001b[0m 14.78   \u001b[0m | \u001b[0m 21.52   \u001b[0m | \u001b[0m 0.03338 \u001b[0m | \u001b[0m 1.585   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.4386  \u001b[0m | \u001b[0m 4.895   \u001b[0m | \u001b[0m 1.608   \u001b[0m | \u001b[0m 343.5   \u001b[0m | \u001b[0m 0.1496  \u001b[0m | \u001b[0m 0.683   \u001b[0m | \u001b[0m 0.2876  \u001b[0m | \u001b[0m 0.4073  \u001b[0m | \u001b[0m 73.51   \u001b[0m | \u001b[0m 1.883   \u001b[0m | \u001b[0m 2.19    \u001b[0m | \u001b[0m 0.467   \u001b[0m | \u001b[0m 96.69   \u001b[0m | \u001b[0m 23.3    \u001b[0m | \u001b[0m 0.9068  \u001b[0m | \u001b[0m 3.485   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8095  \u001b[0m | \u001b[0m 5.332   \u001b[0m | \u001b[0m 6.162   \u001b[0m | \u001b[0m 342.0   \u001b[0m | \u001b[0m 0.8647  \u001b[0m | \u001b[0m 0.4821  \u001b[0m | \u001b[0m 0.1277  \u001b[0m | \u001b[0m 0.2459  \u001b[0m | \u001b[0m 48.92   \u001b[0m | \u001b[0m 1.944   \u001b[0m | \u001b[0m 1.359   \u001b[0m | \u001b[0m 0.1835  \u001b[0m | \u001b[0m 95.42   \u001b[0m | \u001b[0m 69.01   \u001b[0m | \u001b[0m 0.135   \u001b[0m | \u001b[0m 6.495   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.8127  \u001b[0m | \u001b[95m 7.576   \u001b[0m | \u001b[95m 0.475   \u001b[0m | \u001b[95m 362.9   \u001b[0m | \u001b[95m 0.8725  \u001b[0m | \u001b[95m 0.6738  \u001b[0m | \u001b[95m 0.2189  \u001b[0m | \u001b[95m 0.3938  \u001b[0m | \u001b[95m 69.47   \u001b[0m | \u001b[95m 1.421   \u001b[0m | \u001b[95m 2.604   \u001b[0m | \u001b[95m 0.06852 \u001b[0m | \u001b[95m 37.74   \u001b[0m | \u001b[95m 28.66   \u001b[0m | \u001b[95m 0.9722  \u001b[0m | \u001b[95m 6.366   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.562   \u001b[0m | \u001b[0m 6.088   \u001b[0m | \u001b[0m 1.037   \u001b[0m | \u001b[0m 966.4   \u001b[0m | \u001b[0m 0.09171 \u001b[0m | \u001b[0m 0.6298  \u001b[0m | \u001b[0m 0.4612  \u001b[0m | \u001b[0m 0.2359  \u001b[0m | \u001b[0m 72.22   \u001b[0m | \u001b[0m 2.85    \u001b[0m | \u001b[0m 1.748   \u001b[0m | \u001b[0m 0.7737  \u001b[0m | \u001b[0m 90.65   \u001b[0m | \u001b[0m 65.82   \u001b[0m | \u001b[0m 0.1511  \u001b[0m | \u001b[0m 2.624   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.6999  \u001b[0m | \u001b[0m 5.723   \u001b[0m | \u001b[0m 4.132   \u001b[0m | \u001b[0m 625.8   \u001b[0m | \u001b[0m 0.3523  \u001b[0m | \u001b[0m 0.6599  \u001b[0m | \u001b[0m 0.2906  \u001b[0m | \u001b[0m 0.2818  \u001b[0m | \u001b[0m 29.98   \u001b[0m | \u001b[0m 1.825   \u001b[0m | \u001b[0m 1.546   \u001b[0m | \u001b[0m 0.3533  \u001b[0m | \u001b[0m 97.69   \u001b[0m | \u001b[0m 29.4    \u001b[0m | \u001b[0m 0.6828  \u001b[0m | \u001b[0m 0.2227  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7955  \u001b[0m | \u001b[0m 7.518   \u001b[0m | \u001b[0m 6.315   \u001b[0m | \u001b[0m 972.8   \u001b[0m | \u001b[0m 0.5918  \u001b[0m | \u001b[0m 0.2173  \u001b[0m | \u001b[0m 0.3877  \u001b[0m | \u001b[0m 0.1123  \u001b[0m | \u001b[0m 53.5    \u001b[0m | \u001b[0m 1.562   \u001b[0m | \u001b[0m 1.212   \u001b[0m | \u001b[0m 0.4321  \u001b[0m | \u001b[0m 85.65   \u001b[0m | \u001b[0m 19.94   \u001b[0m | \u001b[0m 0.9867  \u001b[0m | \u001b[0m 1.633   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7999  \u001b[0m | \u001b[0m 6.493   \u001b[0m | \u001b[0m 5.948   \u001b[0m | \u001b[0m 262.2   \u001b[0m | \u001b[0m 0.6973  \u001b[0m | \u001b[0m 0.2653  \u001b[0m | \u001b[0m 0.4607  \u001b[0m | \u001b[0m 0.4273  \u001b[0m | \u001b[0m 51.94   \u001b[0m | \u001b[0m 2.641   \u001b[0m | \u001b[0m 1.396   \u001b[0m | \u001b[0m 0.5273  \u001b[0m | \u001b[0m 47.12   \u001b[0m | \u001b[0m 93.33   \u001b[0m | \u001b[0m 0.8254  \u001b[0m | \u001b[0m 3.507   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8097  \u001b[0m | \u001b[0m 1.051   \u001b[0m | \u001b[0m 0.1043  \u001b[0m | \u001b[0m 930.5   \u001b[0m | \u001b[0m 0.5124  \u001b[0m | \u001b[0m 0.8431  \u001b[0m | \u001b[0m 0.1379  \u001b[0m | \u001b[0m 0.4214  \u001b[0m | \u001b[0m 24.98   \u001b[0m | \u001b[0m 2.271   \u001b[0m | \u001b[0m 2.938   \u001b[0m | \u001b[0m 0.4035  \u001b[0m | \u001b[0m 69.36   \u001b[0m | \u001b[0m 29.34   \u001b[0m | \u001b[0m 0.8899  \u001b[0m | \u001b[0m 6.906   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.4351  \u001b[0m | \u001b[0m 6.648   \u001b[0m | \u001b[0m 5.773   \u001b[0m | \u001b[0m 401.7   \u001b[0m | \u001b[0m 0.4758  \u001b[0m | \u001b[0m 0.2681  \u001b[0m | \u001b[0m 0.4473  \u001b[0m | \u001b[0m 0.2068  \u001b[0m | \u001b[0m 97.84   \u001b[0m | \u001b[0m 1.427   \u001b[0m | \u001b[0m 1.596   \u001b[0m | \u001b[0m 0.7394  \u001b[0m | \u001b[0m 11.16   \u001b[0m | \u001b[0m 12.72   \u001b[0m | \u001b[0m 0.7338  \u001b[0m | \u001b[0m 3.488   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8097  \u001b[0m | \u001b[0m 1.963   \u001b[0m | \u001b[0m 0.08037 \u001b[0m | \u001b[0m 880.6   \u001b[0m | \u001b[0m 0.5364  \u001b[0m | \u001b[0m 0.7235  \u001b[0m | \u001b[0m 0.3646  \u001b[0m | \u001b[0m 0.4355  \u001b[0m | \u001b[0m 51.06   \u001b[0m | \u001b[0m 1.852   \u001b[0m | \u001b[0m 2.656   \u001b[0m | \u001b[0m 0.4743  \u001b[0m | \u001b[0m 83.8    \u001b[0m | \u001b[0m 11.28   \u001b[0m | \u001b[0m 0.3968  \u001b[0m | \u001b[0m 0.8984  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.6857  \u001b[0m | \u001b[0m 1.063   \u001b[0m | \u001b[0m 1.854   \u001b[0m | \u001b[0m 798.1   \u001b[0m | \u001b[0m 0.9301  \u001b[0m | \u001b[0m 0.3807  \u001b[0m | \u001b[0m 0.2842  \u001b[0m | \u001b[0m 0.4493  \u001b[0m | \u001b[0m 85.16   \u001b[0m | \u001b[0m 1.05    \u001b[0m | \u001b[0m 2.746   \u001b[0m | \u001b[0m 0.6085  \u001b[0m | \u001b[0m 56.48   \u001b[0m | \u001b[0m 63.04   \u001b[0m | \u001b[0m 0.7998  \u001b[0m | \u001b[0m 1.213   \u001b[0m |\n",
      "| \u001b[95m 16      \u001b[0m | \u001b[95m 0.8136  \u001b[0m | \u001b[95m 1.682   \u001b[0m | \u001b[95m 2.324   \u001b[0m | \u001b[95m 240.9   \u001b[0m | \u001b[95m 0.5526  \u001b[0m | \u001b[95m 0.3518  \u001b[0m | \u001b[95m 0.4668  \u001b[0m | \u001b[95m 0.2906  \u001b[0m | \u001b[95m 28.36   \u001b[0m | \u001b[95m 1.305   \u001b[0m | \u001b[95m 1.547   \u001b[0m | \u001b[95m 0.7837  \u001b[0m | \u001b[95m 45.38   \u001b[0m | \u001b[95m 81.9    \u001b[0m | \u001b[95m 0.4331  \u001b[0m | \u001b[95m 0.5709  \u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "params_nn2 ={\n",
    "    'neurons1': (10, 100),\n",
    "    'neurons2': (10, 100),\n",
    "    'activation1':(0, 9),\n",
    "    'activation2':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(200, 1000),\n",
    "    'epochs':(20, 100),\n",
    "    'layers1':(1,3),\n",
    "    'layers2':(1,3),\n",
    "    'normalization':(0,1),\n",
    "    'dropout1':(0,1),\n",
    "    'dropout_rate1':(0.1,0.5),\n",
    "    'dropout2':(0,1),\n",
    "    'dropout_rate2':(0.1,0.5)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "t = time.time()\n",
    "nn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)\n",
    "nn_bo.maximize(init_points=25, n_iter=4)\n",
    "print(\"elapsed \", time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_nn_ = nn_bo.max['params']\n",
    "learning_rate = params_nn_['learning_rate']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "params_nn_['activation1'] = activationL[round(params_nn_['activation1'])]\n",
    "params_nn_['activation2'] = activationL[round(params_nn_['activation2'])]\n",
    "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
    "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
    "params_nn_['layers1'] = round(params_nn_['layers1'])\n",
    "params_nn_['layers2'] = round(params_nn_['layers2'])\n",
    "params_nn_['neurons1'] = round(params_nn_['neurons1'])\n",
    "params_nn_['neurons2'] = round(params_nn_['neurons2'])\n",
    "params_nn_['normalization'] = round(params_nn_['normalization'])\n",
    "\n",
    "optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),'Adadelta':Adadelta(lr=learning_rate),\n",
    "             'Adagrad':Adagrad(lr=learning_rate), 'RMSprop':RMSprop(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate),\n",
    "             'Adamax':Adamax(lr=learning_rate),'Nadam':Nadam(lr=learning_rate)}\n",
    "params_nn_['optimizer'] = list(optimizerD.keys())[round(params_nn_['optimizer'])]\n",
    "params_nn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7657"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_data['admission_id'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rr', 'ews', 'heart_rate', 'temperature', 'sbp', 'dbp', 'Oxygen_Saturation', 'Assisted_O2', 'Confusion', 'CREA', 'UREA', 'K', 'GFR', 'WBC', 'PLT', 'HCT', 'HGB', 'RBC', 'MCH', 'MCV', 'NEUAB', 'TLYMAB', 'EOSAB', 'MONAB', 'BASAB', 'ALB', 'ALP', 'BILI', 'no_sample_series', 'sex', 'ethnicity', 'age_at_admin', 'Comorb_score', 'Spcfc_Comorb']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_data[1:-1].columns.tolist()[1:-1])\n",
    "len(X_data[1:-1].columns.tolist()[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1102606"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_data[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1102608"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7657*144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39693888"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7657*144*36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time Loading Data: 13.299950122833252\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "##################################################\n",
    "#  O. LOAD DATA AND FEATURE SELECTION\n",
    "##################################################\n",
    "t = time.time()\n",
    "timeSeries      = True\n",
    "peakstroughs    = False\n",
    "samp_to_extract = 0\n",
    "df_patients, df_admissions, df_eobs = load_fn.Load_data()\n",
    "#X_data              = load_fn.Exctract_Xdata(df_patients, df_admissions, df_eobs, samp_to_extract, peakstroughs, timeSeries)\n",
    "X_data= pickle.load(open('timeseries_X_data.pickle','rb'))\n",
    "print(\"Elapsed time Loading Data:\", time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------\n",
    "# Feature Selection, \n",
    "# List of features creation\n",
    "feat_list = X_data.columns.tolist()\n",
    "feat_list = feat_list[1:-1]\n",
    "feat_list = ['rr', 'ews', 'heart_rate', 'temperature', 'sbp', 'dbp', 'Oxygen_Saturation', 'Confusion', 'UREA', \n",
    "             'CREA', 'K', 'GFR', 'sex', 'ethnicity', 'age_at_admin', 'Comorb_score', 'Spcfc_Comorb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_samples: 4618\n",
      "num_val_samples: 2309\n",
      "num_test_samples: 2310\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# PROPORTIONS OF TRAINING, VALIDATION AND TEST SET\n",
    "##################################################\n",
    "list_admins      = X_data['admission_id'].unique().tolist()\n",
    "num_train_admins = int(0.5 * len(list_admins))\n",
    "num_val_admins   = int(0.25 * len(list_admins))\n",
    "num_test_admins  = len(list_admins) - num_train_admins - num_val_admins\n",
    "print(\"num_train_samples:\", num_train_admins)\n",
    "print(\"num_val_samples:\", num_val_admins)\n",
    "print(\"num_test_samples:\", num_test_admins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#  NORMALISATION OF TRAINING SET\n",
    "##################################################\n",
    "raw_data  = X_data[feat_list]\n",
    "mortality = X_data['Mortality'].tolist()\n",
    "\n",
    "mean = raw_data[:num_train_admins * 144].mean(axis=0)\n",
    "std  = raw_data[:num_train_admins * 144].std(axis=0)\n",
    "std  = [1 if s == 0 else s for s in std]\n",
    "\n",
    "norm_data = raw_data - mean\n",
    "norm_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Training, Validation and test sets\n",
    "###################################################\n",
    "\n",
    "train_dataset = norm_data[:num_train_admins * 144]\n",
    "val_dataset   = norm_data[num_train_admins * 144 : ((num_train_admins + num_val_admins) * 144)]\n",
    "test_dataset  = norm_data[((num_train_admins + num_val_admins) * 144):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification Using MiniRocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sktime.datasets import load_arrow_head  # univariate dataset\n",
    "from sktime.transformations.panel.rocket import MiniRocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175, 1) (36, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_arrow_head(split=\"test\", return_X_y=True)\n",
    "X_test, y_test = load_arrow_head(split=\"train\", return_X_y=True)\n",
    "print(X_train.shape, X_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 9996)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "minirocket = MiniRocket() \n",
    "minirocket.fit(X_train)\n",
    "X_train_transform = minirocket.transform(X_train)\n",
    "X_train_transform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifierCV(alphas=array([1.00000000e-03, 4.64158883e-03, 2.15443469e-02, 1.00000000e-01,\n",
       "       4.64158883e-01, 2.15443469e+00, 1.00000000e+01, 4.64158883e+01,\n",
       "       2.15443469e+02, 1.00000000e+03]),\n",
       "                  normalize=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "classifier.fit(X_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_transform = minirocket.transform(X_test)\n",
    "classifier.score(X_test_transform, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAM\n",
    "# https://multithreaded.stitchfix.com/blog/2015/07/30/gam/\n",
    "# https://codeburst.io/pygam-getting-started-with-generalized-additive-models-in-python-457df5b4705f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/minirocket-fast-er-and-accurate-time-series-classification-cdacca2dcbfa\n",
    "#https://towardsdatascience.com/sktime-a-unified-python-library-for-time-series-machine-learning-3c103c139a55\n",
    "#https://pypi.org/project/sktime/\n",
    "#https://arxiv.org/pdf/1302.2277.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8867924528301887"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sktime.datasets import load_arrow_head\n",
    "from sktime.classification.compose import ComposableTimeSeriesForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_arrow_head(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "classifier = ComposableTimeSeriesForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0     -1.531721\n",
       "1     -1.541291\n",
       "2     -1.51496...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0     -1.918966\n",
       "1     -1.894600\n",
       "2     -1.87130...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0     -1.939935\n",
       "1     -1.937687\n",
       "2     -1.91122...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0     -1.813234\n",
       "1     -1.825545\n",
       "2     -1.81662...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0     -1.625142\n",
       "1     -1.622988\n",
       "2     -1.62606...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0     -1.828368\n",
       "1     -1.839300\n",
       "2     -1.80252...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0     -1.890214\n",
       "1     -1.904523\n",
       "2     -1.90426...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0     -1.754535\n",
       "1     -1.777870\n",
       "2     -1.75141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0     -1.632037\n",
       "1     -1.630116\n",
       "2     -1.60748...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0     -2.004295\n",
       "1     -1.985612\n",
       "2     -1.95328...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0     -1.809088\n",
       "1     -1.806718\n",
       "2     -1.78656...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0     -1.756917\n",
       "1     -1.771298\n",
       "2     -1.73085...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0     -1.642492\n",
       "1     -1.637772\n",
       "2     -1.63592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0     -1.897632\n",
       "1     -1.896079\n",
       "2     -1.84324...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0     -2.182975\n",
       "1     -2.126984\n",
       "2     -2.10293...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0     -2.069224\n",
       "1     -2.067731\n",
       "2     -2.04947...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0     -1.774571\n",
       "1     -1.774036\n",
       "2     -1.77658...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0     -1.610578\n",
       "1     -1.609739\n",
       "2     -1.58535...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0     -1.888793\n",
       "1     -1.885038\n",
       "2     -1.85621...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0     -2.063162\n",
       "1     -2.056424\n",
       "2     -2.03189...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0     -2.188812\n",
       "1     -2.185519\n",
       "2     -2.17646...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0     -1.823411\n",
       "1     -1.805534\n",
       "2     -1.79316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0     -1.924509\n",
       "1     -1.920960\n",
       "2     -1.90661...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0     -1.853754\n",
       "1     -1.849193\n",
       "2     -1.83412...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0     -2.041714\n",
       "1     -2.057156\n",
       "2     -2.05215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0     -2.051970\n",
       "1     -2.051526\n",
       "2     -2.00218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0     -1.828865\n",
       "1     -1.815951\n",
       "2     -1.81272...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0     -1.849283\n",
       "1     -1.861474\n",
       "2     -1.82726...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0     -1.653743\n",
       "1     -1.650974\n",
       "2     -1.63187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0     -2.011232\n",
       "1     -2.007302\n",
       "2     -1.98064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0     -1.810890\n",
       "1     -1.793435\n",
       "2     -1.77916...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0     -1.829855\n",
       "1     -1.808497\n",
       "2     -1.79577...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0     -2.053743\n",
       "1     -2.036852\n",
       "2     -2.03303...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0     -1.821392\n",
       "1     -1.801366\n",
       "2     -1.78023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0     -2.097394\n",
       "1     -2.063936\n",
       "2     -1.98190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0     -1.805452\n",
       "1     -1.798470\n",
       "2     -1.76056...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0     -1.947090\n",
       "1     -1.940457\n",
       "2     -1.92236...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0     -1.844180\n",
       "1     -1.839913\n",
       "2     -1.82574...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0     -1.851497\n",
       "1     -1.843622\n",
       "2     -1.82093...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0     -1.941229\n",
       "1     -1.938732\n",
       "2     -1.93526...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0     -1.764156\n",
       "1     -1.785990\n",
       "2     -1.75851...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0     -2.139459\n",
       "1     -2.118865\n",
       "2     -2.10438...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0     -1.960806\n",
       "1     -1.930408\n",
       "2     -1.86434...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0     -1.747085\n",
       "1     -1.729548\n",
       "2     -1.73001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0     -1.823518\n",
       "1     -1.837615\n",
       "2     -1.82742...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0     -1.742703\n",
       "1     -1.739918\n",
       "2     -1.71023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0     -2.012336\n",
       "1     -2.010329\n",
       "2     -1.97935...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0     -1.630727\n",
       "1     -1.629918\n",
       "2     -1.62055...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0     -2.164456\n",
       "1     -2.178541\n",
       "2     -2.06604...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0     -2.130812\n",
       "1     -2.104430\n",
       "2     -2.07475...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0     -1.830178\n",
       "1     -1.812267\n",
       "2     -1.81222...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0     -1.913411\n",
       "1     -1.911633\n",
       "2     -1.89018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0     -2.000005\n",
       "1     -2.002862\n",
       "2     -1.96960...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 dim_0\n",
       "168  0     -1.531721\n",
       "1     -1.541291\n",
       "2     -1.51496...\n",
       "19   0     -1.918966\n",
       "1     -1.894600\n",
       "2     -1.87130...\n",
       "55   0     -1.939935\n",
       "1     -1.937687\n",
       "2     -1.91122...\n",
       "73   0     -1.813234\n",
       "1     -1.825545\n",
       "2     -1.81662...\n",
       "170  0     -1.625142\n",
       "1     -1.622988\n",
       "2     -1.62606...\n",
       "95   0     -1.828368\n",
       "1     -1.839300\n",
       "2     -1.80252...\n",
       "115  0     -1.890214\n",
       "1     -1.904523\n",
       "2     -1.90426...\n",
       "22   0     -1.754535\n",
       "1     -1.777870\n",
       "2     -1.75141...\n",
       "25   0     -1.632037\n",
       "1     -1.630116\n",
       "2     -1.60748...\n",
       "28   0     -2.004295\n",
       "1     -1.985612\n",
       "2     -1.95328...\n",
       "103  0     -1.809088\n",
       "1     -1.806718\n",
       "2     -1.78656...\n",
       "152  0     -1.756917\n",
       "1     -1.771298\n",
       "2     -1.73085...\n",
       "136  0     -1.642492\n",
       "1     -1.637772\n",
       "2     -1.63592...\n",
       "31   0     -1.897632\n",
       "1     -1.896079\n",
       "2     -1.84324...\n",
       "114  0     -2.182975\n",
       "1     -2.126984\n",
       "2     -2.10293...\n",
       "108  0     -2.069224\n",
       "1     -2.067731\n",
       "2     -2.04947...\n",
       "1    0     -1.774571\n",
       "1     -1.774036\n",
       "2     -1.77658...\n",
       "154  0     -1.610578\n",
       "1     -1.609739\n",
       "2     -1.58535...\n",
       "76   0     -1.888793\n",
       "1     -1.885038\n",
       "2     -1.85621...\n",
       "58   0     -2.063162\n",
       "1     -2.056424\n",
       "2     -2.03189...\n",
       "14   0     -2.188812\n",
       "1     -2.185519\n",
       "2     -2.17646...\n",
       "7    0     -1.823411\n",
       "1     -1.805534\n",
       "2     -1.79316...\n",
       "68   0     -1.924509\n",
       "1     -1.920960\n",
       "2     -1.90661...\n",
       "6    0     -1.853754\n",
       "1     -1.849193\n",
       "2     -1.83412...\n",
       "34   0     -2.041714\n",
       "1     -2.057156\n",
       "2     -2.05215...\n",
       "117  0     -2.051970\n",
       "1     -2.051526\n",
       "2     -2.00218...\n",
       "86   0     -1.828865\n",
       "1     -1.815951\n",
       "2     -1.81272...\n",
       "139  0     -1.849283\n",
       "1     -1.861474\n",
       "2     -1.82726...\n",
       "17   0     -1.653743\n",
       "1     -1.650974\n",
       "2     -1.63187...\n",
       "22   0     -2.011232\n",
       "1     -2.007302\n",
       "2     -1.98064...\n",
       "166  0     -1.810890\n",
       "1     -1.793435\n",
       "2     -1.77916...\n",
       "1    0     -1.829855\n",
       "1     -1.808497\n",
       "2     -1.79577...\n",
       "16   0     -2.053743\n",
       "1     -2.036852\n",
       "2     -2.03303...\n",
       "118  0     -1.821392\n",
       "1     -1.801366\n",
       "2     -1.78023...\n",
       "11   0     -2.097394\n",
       "1     -2.063936\n",
       "2     -1.98190...\n",
       "164  0     -1.805452\n",
       "1     -1.798470\n",
       "2     -1.76056...\n",
       "62   0     -1.947090\n",
       "1     -1.940457\n",
       "2     -1.92236...\n",
       "10   0     -1.844180\n",
       "1     -1.839913\n",
       "2     -1.82574...\n",
       "112  0     -1.851497\n",
       "1     -1.843622\n",
       "2     -1.82093...\n",
       "26   0     -1.941229\n",
       "1     -1.938732\n",
       "2     -1.93526...\n",
       "88   0     -1.764156\n",
       "1     -1.785990\n",
       "2     -1.75851...\n",
       "13   0     -2.139459\n",
       "1     -2.118865\n",
       "2     -2.10438...\n",
       "31   0     -1.960806\n",
       "1     -1.930408\n",
       "2     -1.86434...\n",
       "167  0     -1.747085\n",
       "1     -1.729548\n",
       "2     -1.73001...\n",
       "159  0     -1.823518\n",
       "1     -1.837615\n",
       "2     -1.82742...\n",
       "145  0     -1.742703\n",
       "1     -1.739918\n",
       "2     -1.71023...\n",
       "92   0     -2.012336\n",
       "1     -2.010329\n",
       "2     -1.97935...\n",
       "174  0     -1.630727\n",
       "1     -1.629918\n",
       "2     -1.62055...\n",
       "15   0     -2.164456\n",
       "1     -2.178541\n",
       "2     -2.06604...\n",
       "33   0     -2.130812\n",
       "1     -2.104430\n",
       "2     -2.07475...\n",
       "165  0     -1.830178\n",
       "1     -1.812267\n",
       "2     -1.81222...\n",
       "14   0     -1.913411\n",
       "1     -1.911633\n",
       "2     -1.89018...\n",
       "106  0     -2.000005\n",
       "1     -2.002862\n",
       "2     -1.96960..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_0    0     -1.963009\n",
      "1     -1.957825\n",
      "2     -1.95614...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -1.774571\n",
       "1     -1.774036\n",
       "2     -1.776586\n",
       "3     -1.730749\n",
       "4     -1.696268\n",
       "         ...   \n",
       "246   -1.639989\n",
       "247   -1.678683\n",
       "248   -1.729227\n",
       "249   -1.775670\n",
       "250   -1.789324\n",
       "Length: 251, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0     -1.963009\n",
       "1     -1.957825\n",
       "2     -1.95614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0     -1.774571\n",
       "1     -1.774036\n",
       "2     -1.77658...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0     -1.866021\n",
       "1     -1.841991\n",
       "2     -1.83502...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0     -2.073758\n",
       "1     -2.073301\n",
       "2     -2.04460...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0     -1.746255\n",
       "1     -1.741263\n",
       "2     -1.72274...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0     -1.625142\n",
       "1     -1.622988\n",
       "2     -1.62606...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0     -1.657757\n",
       "1     -1.664673\n",
       "2     -1.63264...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0     -1.603279\n",
       "1     -1.587365\n",
       "2     -1.57740...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0     -1.739020\n",
       "1     -1.741534\n",
       "2     -1.73286...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0     -1.630727\n",
       "1     -1.629918\n",
       "2     -1.62055...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 dim_0\n",
       "0    0     -1.963009\n",
       "1     -1.957825\n",
       "2     -1.95614...\n",
       "1    0     -1.774571\n",
       "1     -1.774036\n",
       "2     -1.77658...\n",
       "2    0     -1.866021\n",
       "1     -1.841991\n",
       "2     -1.83502...\n",
       "3    0     -2.073758\n",
       "1     -2.073301\n",
       "2     -2.04460...\n",
       "4    0     -1.746255\n",
       "1     -1.741263\n",
       "2     -1.72274...\n",
       "..                                                 ...\n",
       "170  0     -1.625142\n",
       "1     -1.622988\n",
       "2     -1.62606...\n",
       "171  0     -1.657757\n",
       "1     -1.664673\n",
       "2     -1.63264...\n",
       "172  0     -1.603279\n",
       "1     -1.587365\n",
       "2     -1.57740...\n",
       "173  0     -1.739020\n",
       "1     -1.741534\n",
       "2     -1.73286...\n",
       "174  0     -1.630727\n",
       "1     -1.629918\n",
       "2     -1.62055...\n",
       "\n",
       "[211 rows x 1 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
